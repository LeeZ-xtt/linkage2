# æ ‡å‡†åº“å¯¼å…¥
import os
import random
import argparse
import time

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm

# æœ¬åœ°æ¨¡å—å¯¼å…¥
from config import Config
from utils.scheduler import Scheduler
from utils.index import compute_episode_accuracy, compute_confidence_interval, compute_epoch_statistics, compute_prototype_separation_ratio
from utils.dataloader_improvement import PACSDataset, create_cross_domain_episode_loader, get_pacs_transform  # æ›´æ–°ä¸ºæ”¹è¿›ç‰ˆdataloader
from utils.visualization import (visualize_alpha_weights, plot_epoch_accuracy, plot_epoch_statistics, 
                                plot_training_curve, plot_accuracy_comparison, plot_accuracy_heatmap, plot_val_accuracy_curve, plot_separation_ratio_curve,
                                plot_tsne_embeddings, plot_tsne_evolution, plot_leakindex_curve)
from utils.leakindex import compute_leak_index
from utils.tsne_manager import TSNEVisualizer


def setup_environment(seed):
    """
    è®¾ç½®ç¯å¢ƒç§å­ä»¥ç¡®ä¿å®éªŒå¯é‡å¤æ€§
    
    Args:
        seed (int): éšæœºç§å­å€¼
        
    Note:
        - è®¾ç½®PyTorchã€NumPyã€Python randomçš„ç§å­
        - é…ç½®CUDNNä¸ºç¡®å®šæ€§æ¨¡å¼ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ä½†ä¿è¯å¯é‡å¤æ€§ï¼‰
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    
    # è®¾ç½®cudnnç¡®å®šæ€§è¡Œä¸º
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    

def create_model(config):
    """
    åˆ›å»ºExpB1ModelåŒæµæ¶æ„å¹¶éƒ¨ç½²åˆ°GPU
    
    Args:
        config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ¨¡å‹è¶…å‚æ•°
        
    Returns:
        ExpB1Model: å®éªŒB.1åŒæµæ¨¡å‹å®ä¾‹
    """
    from model.exp_b1_model import ExpB1Model
    
    # åˆ›å»ºExpB1Modelå®ä¾‹å¹¶ç›´æ¥éƒ¨ç½²åˆ°GPU
    model = ExpB1Model(
        n_domains=config.n_domains,  # PACSæœ‰4ä¸ªåŸŸ
        metric=config.metric,         # è·ç¦»åº¦é‡æ–¹å¼
        intrinsic_encoder_drop_rate=config.intrinsic_encoder_drop_rate,
        # æœ¬å¾æ”¯å¯¹æ¯”å­¦ä¹ å‚æ•° (åˆ†æ®µæ¸©åº¦è°ƒåº¦)
        use_intrinsic_supcon=config.use_intrinsic_supcon,
        intrinsic_supcon_weight=config.intrinsic_supcon_weight,
        intrinsic_supcon_proj_dim=config.intrinsic_supcon_proj_dim,
        intrinsic_supcon_dropout=config.intrinsic_supcon_dropout,
        intrinsic_supcon_phase1_temp=config.intrinsic_supcon_phase1_temp,
        intrinsic_supcon_phase2_temp=config.intrinsic_supcon_phase2_temp,
        intrinsic_supcon_phase3_temp=config.intrinsic_supcon_phase3_temp,
        intrinsic_supcon_phase1_end=config.intrinsic_supcon_phase1_end,
        intrinsic_supcon_phase2_start=config.intrinsic_supcon_phase2_start,
        intrinsic_supcon_phase2_end=config.intrinsic_supcon_phase2_end,
        intrinsic_supcon_cross_domain_weight=config.intrinsic_supcon_cross_domain_weight,
        intrinsic_supcon_same_domain_weight=config.intrinsic_supcon_same_domain_weight,
        intrinsic_supcon_cross_domain_neg_weight=config.intrinsic_supcon_cross_domain_neg_weight,
        total_epochs=config.num_epochs,
        # åŸŸæ”¯å¯¹æ¯”å­¦ä¹ å‚æ•° (å€’Uå½¢æ¸©åº¦è°ƒåº¦)
        use_domain_supcon=config.use_domain_supcon,
        domain_supcon_weight=config.domain_supcon_weight,
        domain_supcon_proj_dim=config.domain_supcon_proj_dim,
        domain_supcon_dropout=config.domain_supcon_dropout,
        domain_supcon_early_temp=config.domain_supcon_early_temp,
        domain_supcon_mid_temp=config.domain_supcon_mid_temp,
        domain_supcon_final_temp=config.domain_supcon_final_temp,
        domain_supcon_early_epochs=config.domain_supcon_early_epochs,
        domain_supcon_mid_epochs=config.domain_supcon_mid_epochs,
        domain_supcon_cross_class_weight=config.domain_supcon_cross_class_weight,
        domain_supcon_same_class_weight=config.domain_supcon_same_class_weight,
        domain_supcon_cdsc_neg_weight=config.domain_supcon_cdsc_neg_weight,

        # ==========================
        # åŸŸæ”¯é£æ ¼ç»Ÿè®¡ç¼–ç å™¨ï¼ˆæ–¹æ¡ˆAï¼‰
        # ==========================
        domain_dim=config.domain_dim,
        domain_filterbank_kernel=config.domain_filterbank_kernel,
        domain_filterbank_use_gabor=config.domain_filterbank_use_gabor,
        domain_filterbank_use_dog=config.domain_filterbank_use_dog,
        domain_filterbank_trainable_gain=config.domain_filterbank_trainable_gain,
        domain_filterbank_trainable_delta_kernel=config.domain_filterbank_trainable_delta_kernel,
        domain_token_dropout=config.domain_token_dropout,
        domain_token_scale_init=config.domain_token_scale_init,
        domain_use_gram_stats=config.domain_use_gram_stats,
        domain_gram_rank=config.domain_gram_rank,
        domain_gram_out_dim=config.domain_gram_out_dim,
        domain_norm=config.domain_norm,
        # ==========================
        # SAP æ¨¡å—é…ç½®
        # ==========================
        use_sap=config.use_sap,
        sap_dropout=config.sap_dropout,
        sap_orth_weight=config.sap_orth_weight,
        k_shot=config.k_shot,
    )
    
    # éƒ¨ç½²åˆ°è®¾å¤‡
    device = Config.device
    model = model.to(device=device)
    
    # åº”ç”¨channels_lastå†…å­˜æ ¼å¼ä¼˜åŒ–ï¼ˆå¯æå‡20-30%æ€§èƒ½ï¼‰
    channels_last_enabled = False
    if torch.cuda.is_available():
        try:
            model = model.to(memory_format=torch.channels_last)
            channels_last_enabled = True
            print("âœ… æ¨¡å‹å·²è½¬æ¢ä¸ºchannels_lastå†…å­˜æ ¼å¼ (é¢„æœŸæ€§èƒ½æå‡20-30%)")
        except RuntimeError as e:
            print(f"âš ï¸  channels_lastè½¬æ¢å¤±è´¥: {e}")
            print("   ç»§ç»­ä½¿ç”¨é»˜è®¤contiguousæ ¼å¼")
    else:
        print("âš ï¸  CPUæ¨¡å¼ä¸æ”¯æŒchannels_lastä¼˜åŒ–")
    
    print(f"ğŸ“‹ å·²åˆ›å»ºæ¨¡å‹: {model.__class__.__name__}")
    print(f"ğŸš€ æ¨¡å‹å·²éƒ¨ç½²åˆ°: {device}")
    print(f"ğŸ”§ æ¨¡å‹é…ç½®: n_domains={config.n_domains}, metric={config.metric}")
    print(f"ğŸ”— æœ¬å¾æ”¯SupCon: use={config.use_intrinsic_supcon}, weight={config.intrinsic_supcon_weight}")
    print(f"ğŸ”— åŸŸæ”¯DomSupCon: use={config.use_domain_supcon}, weight={config.domain_supcon_weight}")
    print(f"ğŸ”— SAPå‡€åŒ–æ¨¡å—: use={config.use_sap}, orth_weight={config.sap_orth_weight}")
    
    return model, channels_last_enabled


def run_episode(model, support_images, support_labels, query_images, n_way, 
                query_domain_labels=None, query_labels=None, support_domain_labels=None):
    """
    è¿è¡Œå•ä¸ªepisode - ExpB1ModelåŒæµæ¶æ„ + åŒå‘å¯¹æ¯”å­¦ä¹  + SAPå‡€åŒ–
    
    Args:
        model: ExpB1Modelå®ä¾‹
        support_images: æ”¯æŒé›†å›¾åƒ [B, C, H, W]
        support_labels: æ”¯æŒé›†æ ‡ç­¾ [B]
        query_images: æŸ¥è¯¢é›†å›¾åƒ [B, C, H, W]
        n_way: ç±»åˆ«æ•°
        query_domain_labels: æŸ¥è¯¢é›†åŸŸæ ‡ç­¾ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        query_labels: æŸ¥è¯¢é›†ç±»åˆ«æ ‡ç­¾ï¼ˆç”¨äºSupConæŸå¤±ï¼‰
        support_domain_labels: æ”¯æŒé›†åŸŸæ ‡ç­¾ï¼ˆç”¨äºSupConè·¨åŸŸæƒé‡ï¼‰
        
    Returns:
        tuple: (logits, prototypes, domain_logits,
                intrinsic_supcon_loss, intrinsic_supcon_stats, 
                domain_supcon_loss, domain_supcon_stats,
                sap_loss, sap_stats)

    Note:
        åŒæµæ¶æ„è¿”å›: åˆ†ç±»logits, åŸå‹, åŸŸåˆ†ç±»logits,
                     æœ¬å¾æ”¯SupConæŸå¤±, æœ¬å¾æ”¯SupConç»Ÿè®¡, 
                     åŸŸæ”¯SupConæŸå¤±, åŸŸæ”¯SupConç»Ÿè®¡,
                     SAPæ­£äº¤æŸå¤±, SAPç»Ÿè®¡
    """
    return model(support_images, support_labels, query_images, n_way, 
                 query_domain_labels, query_labels, support_domain_labels)


# ä½¿ç”¨ utils.index ä¸­çš„ compute_epoch_statisticsï¼Œç§»é™¤æœ¬åœ°é‡å¤å®ç°


def evaluate_model(model, dataset, config, num_test_episodes=100):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.set_mode('eval')
    
    accuracies = []
    
    # åˆ›å»ºæµ‹è¯•episodeåŠ è½½å™¨ - ä½¿ç”¨è·¨åŸŸé‡‡æ ·
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ args.test_source_domains å’Œ args.test_query_domains
    # ä½† evaluate_model å‡½æ•°ç­¾åæ²¡æœ‰è¿™äº›å‚æ•°ï¼Œæˆ‘ä»¬éœ€è¦ä» config è·å–
    episode_loader = create_cross_domain_episode_loader(
        dataset, config.n_way, config.k_shot, 
        config.query_per_class, num_test_episodes,
        support_domain_pool=config.test_source_domains,
        query_domain_pool=config.test_query_domains
    )
    
    use_amp = getattr(Config, 'use_amp', False) and torch.cuda.is_available()
    amp_dtype = torch.bfloat16 if getattr(Config, 'amp_dtype', 'bf16').lower() == 'bf16' else torch.float16
    with torch.no_grad():
        for episode_idx, episode_data in enumerate(episode_loader):
            if len(episode_data) == 6:
                support_images, support_labels, query_images, query_labels, _, _ = episode_data
            else:
                support_images, support_labels, query_images, query_labels = episode_data[:4]
            # ç§»åŠ¨åˆ°è®¾å¤‡
            support_images = support_images.to(config.device)
            support_labels = support_labels.to(config.device)
            query_images = query_images.to(config.device)
            query_labels = query_labels.to(config.device)
            
            if use_amp:
                with torch.amp.autocast('cuda', dtype=amp_dtype):
                    logits = run_episode(
                        model, support_images, support_labels,
                        query_images, config.n_way, query_domain_labels=None
                    )[0]
            else:
                logits = run_episode(
                    model, support_images, support_labels,
                    query_images, config.n_way, query_domain_labels=None
                )[0]
            
            # è®¡ç®—å‡†ç¡®ç‡
            acc = compute_episode_accuracy(logits, query_labels)
            accuracies.append(acc)
            
            if (episode_idx + 1) % 10 == 0:
                print(f"  Evaluated {episode_idx + 1}/{num_test_episodes} episodes")
    
    # è®¡ç®—å¹³å‡å‡†ç¡®ç‡å’Œç½®ä¿¡åŒºé—´
    mean_acc = np.mean(accuracies)
    lower_bound, upper_bound = compute_confidence_interval(accuracies)
    
    # é‡ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    model.set_mode('train')
    
    return mean_acc, lower_bound, upper_bound


def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•° - æ›´æ–°ä¸ºPACSæ•°æ®é›†è·¯å¾„
    parser = argparse.ArgumentParser(description='Train ExpB1Model - Dual Stream + HSIC for Domain Generalization (Improved Sampling)')
    parser.add_argument('--pacs_root', type=str, required=True, help='Path to PACS dataset root directory')
    
    # è®­ç»ƒé˜¶æ®µåŸŸé…ç½®
    parser.add_argument('--train_source_domains', nargs='+', default=['photo', 'art_painting', 'cartoon'], 
                       help='Source domains for training support set')
    parser.add_argument('--train_query_domains', nargs='+', default=['photo', 'art_painting', 'cartoon'], 
                       help='Domains for training query set (will be sampled consistently per episode)')
                       
    # éªŒè¯é˜¶æ®µåŸŸé…ç½®
    parser.add_argument('--test_source_domains', nargs='+', default=['photo', 'art_painting', 'cartoon'], 
                       help='Source domains for validation support set')
    parser.add_argument('--test_query_domains', nargs='+', default=['sketch'], 
                       help='Target domain for validation query set')
                       
    parser.add_argument('--num_epochs', type=int, default=Config.num_epochs, help='Number of epochs to train')
    parser.add_argument('--episodes_per_epoch', type=int, default=Config.episodes_per_epoch, help='Number of episodes per epoch')
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    Config.num_epochs = args.num_epochs
    Config.episodes_per_epoch = args.episodes_per_epoch
    
    # å°†å‚æ•°æ³¨å…¥ Config ä»¥ä¾¿åœ¨ evaluate_model ä¸­ä½¿ç”¨
    Config.train_source_domains = args.train_source_domains
    Config.train_query_domains = args.train_query_domains
    Config.test_source_domains = args.test_source_domains
    Config.test_query_domains = args.test_query_domains
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment(Config.seed)
    
    # æ‰“å°è®¾å¤‡ä¿¡æ¯
    Config.print_device_info()
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆå·²åŒ…å«GPUéƒ¨ç½²ï¼‰- ä½¿ç”¨å®Œæ•´é…ç½®
    model, channels_last_enabled = create_model(Config)
    
    # è·å–æ¨¡å‹å‚æ•°ç»„ï¼ˆPatch 3: æ”¯æŒå‚æ•°åˆ†ç»„ï¼‰
    param_groups = model.get_parameters()
    
    # ä¼˜åŒ–å™¨é…ç½® - ä½¿ç”¨é…ç½®å‚æ•°
    optimizer = torch.optim.SGD(param_groups, lr=Config.learning_rate, 
                                momentum=Config.momentum, weight_decay=Config.weight_decay,
                                nesterov=Config.nesterov)
    
    # æå–æ‰€æœ‰å‚æ•°ç”¨äºæ¢¯åº¦è£å‰ªï¼ˆä»å‚æ•°ç»„ä¸­å±•å¼€ï¼‰
    all_params = []
    for group in param_groups:
        all_params.extend(group['params'])
    
    # ç»„åˆè°ƒåº¦å™¨ï¼šçº¿æ€§é¢„çƒ­ + MultiStepLR
    scheduler = Scheduler(optimizer)
    
    # æŸå¤±å‡½æ•°é…ç½®
    criterion = nn.CrossEntropyLoss()
    domain_criterion = nn.CrossEntropyLoss()

    use_amp = getattr(Config, 'use_amp', False) and torch.cuda.is_available()
    amp_dtype = torch.bfloat16 if getattr(Config, 'amp_dtype', 'bf16').lower() == 'bf16' else torch.float16
    scaler = torch.amp.GradScaler('cuda', enabled=use_amp and amp_dtype == torch.float16)
    
    print(f"ğŸ”§ è®­ç»ƒé…ç½®:")
    print(f"   å­¦ä¹ ç‡: {Config.learning_rate} (é¢„çƒ­: {Config.warmup_epochs} epochs)")
    print(f"   æŸå¤±æƒé‡ - Domain_loss: {Config.domain_loss_weight}")
    print(f"   æ¢¯åº¦è£å‰ª: {Config.grad_clip_norm}")
    print(f"   éªŒè¯é¢‘ç‡: æ¯ {Config.eval_frequency} epochs")
    print(f"   AMP: {use_amp} (dtype={getattr(Config, 'amp_dtype', 'bf16')})")
    
    # æ•°æ®é¢„å¤„ç† - ä½¿ç”¨PACSä¸“ç”¨transform
    train_transform = get_pacs_transform(image_size=84, split='train')
    eval_transform = get_pacs_transform(image_size=84, split='test')
    
    # åŠ è½½PACSæ•°æ®é›† - åŸŸæ³›åŒ–è®¾ç½®
    # è®­ç»ƒé›†éœ€è¦åŒ…å«æ‰€æœ‰è®­ç»ƒé˜¶æ®µç”¨åˆ°çš„åŸŸ
    train_domains = list(set(args.train_source_domains + args.train_query_domains))
    train_dataset = PACSDataset(
        root_dir=args.pacs_root,
        target_domains=train_domains,
        split='train',
        transform=train_transform
    )
    
    # éªŒè¯é›†éœ€è¦åŒ…å«æ‰€æœ‰éªŒè¯é˜¶æ®µç”¨åˆ°çš„åŸŸ
    val_domains = list(set(args.test_source_domains + args.test_query_domains))
    val_dataset = PACSDataset(
        root_dir=args.pacs_root,
        target_domains=val_domains,
        split='test',  # PACSæ²¡æœ‰ç‹¬ç«‹çš„valé›†ï¼Œä½¿ç”¨testä½œä¸ºéªŒè¯
        transform=eval_transform
    )

    # T-SNE ä¸“ç”¨å›ºå®šæ•°æ®é›† (ä½¿ç”¨eval_transformé¿å…éšæœºå¢å¼ºï¼Œç¡®ä¿å¯è§†åŒ–ä¸€è‡´æ€§)
    # æ³¨æ„ï¼šä¸ºäº†å¯¹é½åŸŸæ³›åŒ–è¯„ä¼°ï¼ˆsourceâ†’targetï¼‰ï¼Œéœ€è¦åŒ…å«test_source_domainså’Œtest_query_domains
    tsne_domains = list(set(args.test_source_domains + args.test_query_domains))
    tsne_dataset = PACSDataset(
        root_dir=args.pacs_root,
        target_domains=tsne_domains,  # ä½¿ç”¨éªŒè¯é˜¶æ®µçš„åŸŸï¼ˆsource + targetï¼‰
        split='train',
        transform=eval_transform      # å…³é”®ï¼šä½¿ç”¨è¯„ä¼°æ—¶çš„å˜æ¢ï¼ˆæ— éšæœºå¢å¼ºï¼‰
    )
    
    print(f"ğŸ“Š æ•°æ®é›†é…ç½® (Improved):")
    print(f"   è®­ç»ƒæ”¯æŒåŸŸ: {args.train_source_domains}")
    print(f"   è®­ç»ƒæŸ¥è¯¢åŸŸ: {args.train_query_domains}")
    print(f"   éªŒè¯æ”¯æŒåŸŸ: {args.test_source_domains}")
    print(f"   éªŒè¯æŸ¥è¯¢åŸŸ: {args.test_query_domains}")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)} å¼ å›¾åƒ (æ¶µç›– {train_domains})")
    print(f"   éªŒè¯æ ·æœ¬: {len(val_dataset)} å¼ å›¾åƒ (æ¶µç›– {val_domains})")
    
    # è®­ç»ƒå†å²è®°å½•
    train_losses = []
    train_accuracies = []
    val_accuracies = []
    epoch_stds = []  # ç”¨äºè®°å½•æ¯ä¸ªepochçš„æ ‡å‡†å·®
    
    # çƒ­åŠ›å›¾æ•°æ®æ”¶é›†ï¼šä¸åŒé˜¶æ®µçš„å‡†ç¡®ç‡çŸ©é˜µ
    heatmap_data = []
    sep_ratio_curve = []
    sep_ratio_epochs = []
    best_val_acc = 0.0
    
    # T-SNEå¯è§†åŒ–æ•°æ®æ”¶é›†
    tsne_metrics_history = []
    tsne_epochs = []
    tsne_save_dir = "figures/T-SNE"
    os.makedirs(tsne_save_dir, exist_ok=True)
    print(f"ğŸ“Š T-SNEå¯è§†åŒ–å°†ä¿å­˜åˆ°: {tsne_save_dir} (æ¯5ä¸ªepoch)")

    # åˆå§‹åŒ–å›ºå®šEpisodeçš„T-SNEå¯è§†åŒ–å™¨
    # å¯¹é½åŸŸæ³›åŒ–è¯„ä¼°è®¾ç½®ï¼šsupportæ¥è‡ªsourceåŸŸï¼Œqueryæ¥è‡ªtargetåŸŸï¼ˆsketchï¼‰
    tsne_visualizer = TSNEVisualizer(
        dataset=tsne_dataset,
        n_way=Config.n_way,
        k_shot=Config.k_shot,
        query_per_class=Config.query_per_class,
        num_episodes=3,  # å›ºå®š3ä¸ªepisodeç”¨äºå¯è§†åŒ–
        support_domain_pool=Config.test_source_domains,  # ä½¿ç”¨éªŒè¯é˜¶æ®µçš„sourceåŸŸ
        query_domain_pool=Config.test_query_domains,      # ä½¿ç”¨éªŒè¯é˜¶æ®µçš„targetåŸŸï¼ˆsketchï¼‰
        device=Config.device
    )
    
    # ===== å¯¹æ¯”å­¦ä¹ ç»Ÿè®¡æŒ‡æ ‡å†å²è®°å½• =====
    # æœ¬å¾æ”¯æŒ‡æ ‡
    intrinsic_pos_sim_history = []      # åŒç±»æ­£æ ·æœ¬ç›¸ä¼¼åº¦
    intrinsic_neg_sim_history = []      # å¼‚ç±»è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
    intrinsic_cross_domain_sim_history = []  # è·¨åŸŸåŒç±»ç›¸ä¼¼åº¦
    # åŸŸæ”¯æŒ‡æ ‡
    domain_cross_class_sim_history = []  # SDDC: åŒåŸŸå¼‚ç±» (æ ¸å¿ƒ)
    domain_same_class_sim_history = []   # SDSC: åŒåŸŸåŒç±»
    domain_cdsc_sim_history = []         # CDSC: è·¨åŸŸåŒç±» (æ³„éœ²æ£€æµ‹)
    domain_cddc_sim_history = []         # CDDC: è·¨åŸŸå¼‚ç±» (çº¯è´Ÿæ ·æœ¬)
    
    # ===== LeakIndex å†å²è®°å½• (æ–°å¢) =====
    leakindex_history = []               # LeakIndex = CDSC - CDDC (å¸¦ç¬¦å·)
    leakintensity_history = []           # LeakIntensity = max(0, LeakIndex) (ä»…æ­£å€¼)
    


    print("Starting training...")
    
    # æ£€æŸ¥eval_frequencyå‚æ•°çš„æœ‰æ•ˆæ€§
    if not isinstance(Config.eval_frequency, int) or Config.eval_frequency < 1:
        raise ValueError("eval_frequency must be a positive integer")
    
    # è®°å½•æ€»è®­ç»ƒå¼€å§‹æ—¶é—´
    total_start_time = time.time()
    epoch_times = []  # è®°å½•æ¯ä¸ªepochçš„æ—¶é—´
    
    # è®­ç»ƒå¾ªç¯
    for epoch in tqdm(range(Config.num_epochs), desc="Training Progress", unit="epoch"):
        # è®°å½•epochå¼€å§‹æ—¶é—´
        epoch_start_time = time.time()
        print(f"\nEpoch {epoch+1}/{Config.num_epochs}")
        # åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶æ›´æ–°å­¦ä¹ ç‡ï¼ˆæ»¡è¶³é¢„çƒ­é˜¶æ®µçº¿æ€§å¢é•¿ä¸ä¸»é˜¶æ®µé‡Œç¨‹ç¢‘ä¸‹é™çš„è¦æ±‚ï¼‰
        scheduler.step()
        print(f"  Current LR: {scheduler.get_lr():.6f}")
        
        # åˆ›å»ºè®­ç»ƒepisodeåŠ è½½å™¨ - ä½¿ç”¨æ”¹è¿›çš„è·¨åŸŸé‡‡æ ·ç­–ç•¥
        episode_loader = create_cross_domain_episode_loader(
            train_dataset, Config.n_way, Config.k_shot, 
            Config.query_per_class, Config.episodes_per_epoch,
            support_domain_pool=args.train_source_domains,
            query_domain_pool=args.train_query_domains
        )
        
        epoch_losses = []
        epoch_accuracies = []
        epoch_domain_losses = []  # è®°å½•åŸŸåˆ†ç±»æŸå¤±
        epoch_domain_accuracies = [] # è®°å½•åŸŸåˆ†ç±»å‡†ç¡®ç‡
        epoch_intrinsic_supcon_losses = []  # è®°å½•æœ¬å¾æ”¯SupConæŸå¤±
        epoch_domain_supcon_losses = []  # è®°å½•åŸŸæ”¯SupConæŸå¤±
        epoch_cross_domain_sim = []  # è®°å½•è·¨åŸŸæ­£æ ·æœ¬ç›¸ä¼¼åº¦
        episode_times = []
        
        # ===== å¯¹æ¯”å­¦ä¹ ç»Ÿè®¡æŒ‡æ ‡ (epochçº§åˆ«) =====
        # æœ¬å¾æ”¯æŒ‡æ ‡
        epoch_intrinsic_pos_sim = []     # åŒç±»æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        epoch_intrinsic_neg_sim = []     # å¼‚ç±»è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        # åŸŸæ”¯æŒ‡æ ‡
        epoch_domain_cross_class_sim = []  # SDDCï¼ˆåŒåŸŸå¼‚ç±»ç›¸ä¼¼åº¦ï¼Œæ ¸å¿ƒåæ˜ ç±»é—´åˆ†ç¦»åº¦ï¼Œåº”è¶Šé«˜è¶Šå¥½ï¼‰
        epoch_domain_same_class_sim = []   # SDSCï¼ˆåŒåŸŸåŒç±»ç›¸ä¼¼åº¦ï¼Œè¡¡é‡ç±»å†…èšåˆæ€§ï¼Œåº”è¶Šé«˜è¶Šå¥½ï¼‰
        epoch_domain_cdsc_sim = []         # CDSCï¼ˆè·¨åŸŸåŒç±»ç›¸ä¼¼åº¦ï¼Œå…³é”®æŒ‡æ ‡ï¼Œç›‘æ§ç±»åˆ«ä¿¡æ¯æ˜¯å¦æ³„éœ²ï¼Œåº”è¶Šä½è¶Šå¥½ï¼Œä¸CDDCæ¥è¿‘ï¼‰
        epoch_domain_cddc_sim = []         # CDDCï¼ˆè·¨åŸŸå¼‚ç±»ç›¸ä¼¼åº¦ï¼Œçº¯è´Ÿæ ·æœ¬ï¼Œç†æƒ³æƒ…å†µä¸‹åº”è¶Šä½è¶Šå¥½ï¼‰
        # LeakIndex æŒ‡æ ‡ (æ–°å¢)
        epoch_leak_list = []               # æ¯ä¸ª episode çš„ LeakIndex
        # SAP ç»Ÿè®¡æŒ‡æ ‡ (æ–°å¢)
        epoch_sap_gate = []                # SAP å‡€åŒ–å¼ºåº¦
        epoch_sap_attn_max = []            # SAP æœ€å¤§æ³¨æ„åŠ›å‡å€¼
        epoch_sap_purif_ratio = []         # SAP å‡€åŒ–æ¯”ä¾‹
        epoch_sap_w_orth_norm = []         # SAP W_orth æƒé‡èŒƒæ•°
        layer_weights = None
        epoch_sep_ratios = [] if (epoch + 1) % 5 == 0 else None
        
        # è®¾ç½®å½“å‰epochç”¨äºSupConæ¸©åº¦è°ƒåº¦
        model.set_epoch(epoch)
        
        # éå†æ‰€æœ‰episodes
        episode_loader_with_progress = tqdm(episode_loader, 
                                          total=Config.episodes_per_epoch,
                                          desc=f"Epoch {epoch+1} Episodes", 
                                          leave=False, 
                                          unit="episode")
        
        for episode_idx, episode_data in enumerate(episode_loader_with_progress):
            # å¤„ç†ä¸åŒé•¿åº¦çš„episodeæ•°æ®ï¼ˆå…¼å®¹åŸŸæ ‡ç­¾ï¼‰
            if len(episode_data) == 6:
                support_images, support_labels, query_images, query_labels, support_domains, query_domains = episode_data
                query_domain_labels = query_domains.to(Config.device)
                support_domain_labels = support_domains.to(Config.device)  # æ–°å¢: æ”¯æŒé›†åŸŸæ ‡ç­¾
            else:
                support_images, support_labels, query_images, query_labels = episode_data[:4]
                query_domain_labels = None  # æ²¡æœ‰åŸŸæ ‡ç­¾æ—¶ä½¿ç”¨None
                support_domain_labels = None
            
            # è®°å½•episodeå¼€å§‹æ—¶é—´
            episode_start_time = time.time()
            
            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶åº”ç”¨channels_lastæ ¼å¼ï¼ˆè‹¥å·²å¯ç”¨ï¼‰
            if channels_last_enabled:
                support_images = support_images.to(Config.device, memory_format=torch.channels_last)
                query_images = query_images.to(Config.device, memory_format=torch.channels_last)
            else:
                support_images = support_images.to(Config.device)
                query_images = query_images.to(Config.device)
            
            support_labels = support_labels.to(Config.device)
            query_labels = query_labels.to(Config.device)

            if support_images.dim() != 4 or query_images.dim() != 4:
                raise ValueError(f"Expected 4D images, got support {support_images.dim()}D, query {query_images.dim()}D")
            
            # å‰å‘ä¸æŸå¤±è®¡ç®—ï¼ˆAMPï¼‰
            if use_amp:
                with torch.amp.autocast('cuda', dtype=amp_dtype):
                    (logits, prototypes, domain_logits,
                     intrinsic_supcon_loss, intrinsic_supcon_stats, 
                     domain_supcon_loss, domain_supcon_stats,
                     sap_loss, sap_stats) = run_episode(
                        model, support_images, support_labels,
                        query_images, Config.n_way, query_domain_labels, query_labels, support_domain_labels
                    )
                    cls_loss = criterion(logits, query_labels)
                    domain_loss = torch.zeros((), device=Config.device, dtype=logits.dtype)
                    domain_acc = 0.0
                    if domain_logits is not None and query_domain_labels is not None:
                        # æ£€æŸ¥ logits æ˜¯å¦åŒ…å« Support+Query (N_s + N_q)
                        if domain_logits.size(0) > query_domain_labels.size(0):
                            if support_domain_labels is not None:
                                all_domain_labels = torch.cat([support_domain_labels, query_domain_labels], dim=0)
                                domain_loss = domain_criterion(domain_logits, all_domain_labels)
                                _, domain_preds = torch.max(domain_logits, 1)
                                domain_acc = (domain_preds == all_domain_labels).float().mean().item()
                            else:
                                # å¼‚å¸¸æƒ…å†µï¼šLogits å˜å¤§äº†ä½†æ²¡æœ‰ Support æ ‡ç­¾ï¼Œå›é€€åˆ°åˆ‡ç‰‡
                                # (ç†è®ºä¸Š forward å†…éƒ¨æ§åˆ¶äº†ï¼Œè¿™é‡Œåšé˜²å¾¡æ€§ç¼–ç¨‹)
                                q_len = query_domain_labels.size(0)
                                domain_loss = domain_criterion(domain_logits[-q_len:], query_domain_labels)
                                _, domain_preds = torch.max(domain_logits[-q_len:], 1)
                                domain_acc = (domain_preds == query_domain_labels).float().mean().item()
                        else:
                            # ä»… Query
                            domain_loss = domain_criterion(domain_logits, query_domain_labels)
                            _, domain_preds = torch.max(domain_logits, 1)
                            domain_acc = (domain_preds == query_domain_labels).float().mean().item()
                    
                    # è®¡ç®—æ€»æŸå¤± (åŒ…å«åŒå‘å¯¹æ¯”å­¦ä¹  + SAPæ­£äº¤æŸå¤±)
                    intrinsic_supcon_weight = Config.intrinsic_supcon_weight
                    domain_supcon_weight = Config.domain_supcon_weight
                    sap_orth_weight = Config.sap_orth_weight
                    total_loss = (
                        cls_loss +
                        Config.domain_loss_weight * domain_loss +
                        intrinsic_supcon_weight * intrinsic_supcon_loss +
                        domain_supcon_weight * domain_supcon_loss +
                        sap_orth_weight * sap_loss
                    )
            else:
                (logits, prototypes, domain_logits,
                 intrinsic_supcon_loss, intrinsic_supcon_stats, 
                 domain_supcon_loss, domain_supcon_stats,
                 sap_loss, sap_stats) = run_episode(
                    model, support_images, support_labels,
                    query_images, Config.n_way, query_domain_labels, query_labels, support_domain_labels
                )
                cls_loss = criterion(logits, query_labels)
                domain_loss = torch.zeros((), device=Config.device, dtype=logits.dtype)
                domain_acc = 0.0
                if domain_logits is not None and query_domain_labels is not None:
                    # æ£€æŸ¥ logits æ˜¯å¦åŒ…å« Support+Query (N_s + N_q)
                    if domain_logits.size(0) > query_domain_labels.size(0):
                        if support_domain_labels is not None:
                            all_domain_labels = torch.cat([support_domain_labels, query_domain_labels], dim=0)
                            domain_loss = domain_criterion(domain_logits, all_domain_labels)
                            _, domain_preds = torch.max(domain_logits, 1)
                            domain_acc = (domain_preds == all_domain_labels).float().mean().item()
                        else:
                            # å¼‚å¸¸æƒ…å†µï¼šLogits å˜å¤§äº†ä½†æ²¡æœ‰ Support æ ‡ç­¾ï¼Œå›é€€åˆ°åˆ‡ç‰‡
                            q_len = query_domain_labels.size(0)
                            domain_loss = domain_criterion(domain_logits[-q_len:], query_domain_labels)
                            _, domain_preds = torch.max(domain_logits[-q_len:], 1)
                            domain_acc = (domain_preds == query_domain_labels).float().mean().item()
                    else:
                        # ä»… Query
                        domain_loss = domain_criterion(domain_logits, query_domain_labels)
                        _, domain_preds = torch.max(domain_logits, 1)
                        domain_acc = (domain_preds == query_domain_labels).float().mean().item()
                
                # è®¡ç®—æ€»æŸå¤± (åŒ…å«åŒå‘å¯¹æ¯”å­¦ä¹  + SAPæ­£äº¤æŸå¤±)
                intrinsic_supcon_weight = Config.intrinsic_supcon_weight
                domain_supcon_weight = Config.domain_supcon_weight
                sap_orth_weight = Config.sap_orth_weight
                total_loss = (
                    cls_loss +
                    Config.domain_loss_weight * domain_loss +
                    intrinsic_supcon_weight * intrinsic_supcon_loss +
                    domain_supcon_weight * domain_supcon_loss +
                    sap_orth_weight * sap_loss
                )

            # å¢å¼ºçš„NaNæ£€æµ‹ï¼šæ£€æŸ¥æ‰€æœ‰æŸå¤±ç»„ä»¶å¹¶æä¾›è¯¦ç»†è¯Šæ–­ä¿¡æ¯
            if (torch.isnan(cls_loss) or torch.isnan(domain_loss) or
                torch.isnan(intrinsic_supcon_loss) or torch.isnan(domain_supcon_loss) or
                torch.isnan(sap_loss)):
                print(f"\nâŒ NaNæŸå¤±æ£€æµ‹åˆ°åœ¨Epoch {epoch+1}, Episode {episode_idx+1}:")
                print(f"   åˆ†ç±»æŸå¤±(cls_loss): {cls_loss.item()}")
                print(f"   åŸŸåˆ†ç±»æŸå¤±(domain_loss): {domain_loss.item()}")
                print(f"   æœ¬å¾SupConæŸå¤±(intrinsic_supcon_loss): {intrinsic_supcon_loss.item()}")
                print(f"   åŸŸSupConæŸå¤±(domain_supcon_loss): {domain_supcon_loss.item()}")
                print(f"   SAPæ­£äº¤æŸå¤±(sap_loss): {sap_loss.item()}")
                print(f"   å½“å‰å­¦ä¹ ç‡: {scheduler.get_lr():.6f}")
                raise ValueError("æŸå¤±ç»„ä»¶åŒ…å«NaNï¼Œè®­ç»ƒç»ˆæ­¢")
            
            if torch.isnan(total_loss):
                raise ValueError(f"æ€»æŸå¤±ä¸ºNaN (total_loss={total_loss.item()})")

            optimizer.zero_grad(set_to_none=True)
            if scaler.is_enabled():
                scaler.scale(total_loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(all_params, max_norm=Config.grad_clip_norm)
                
                # æ¢¯åº¦ç›‘æ§ï¼šæ£€æµ‹æ¢¯åº¦çˆ†ç‚¸
                total_grad_norm = 0.0
                for p in all_params:
                    if p.grad is not None:
                        param_norm = p.grad.data.norm(2)
                        total_grad_norm += param_norm.item() ** 2
                total_grad_norm = total_grad_norm ** 0.5
                
                # è­¦å‘Šï¼šæ¢¯åº¦å¼‚å¸¸å¤§ï¼ˆè¶…è¿‡10å€é˜ˆå€¼ï¼‰
                if total_grad_norm > Config.grad_clip_norm * 10:
                    print(f"\nâš ï¸  æ¢¯åº¦å¼‚å¸¸å¤§è­¦å‘Š - Epoch {epoch+1}, Episode {episode_idx+1}:")
                    print(f"   æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.2f} (é˜ˆå€¼: {Config.grad_clip_norm})")
                    print(f"   å½“å‰å­¦ä¹ ç‡: {scheduler.get_lr():.6f}")
                
                scaler.step(optimizer)
                scaler.update()
            else:
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(all_params, max_norm=Config.grad_clip_norm)
                
                # æ¢¯åº¦ç›‘æ§ï¼šæ£€æµ‹æ¢¯åº¦çˆ†ç‚¸ï¼ˆéAMPè·¯å¾„ï¼‰
                total_grad_norm = 0.0
                for p in all_params:
                    if p.grad is not None:
                        param_norm = p.grad.data.norm(2)
                        total_grad_norm += param_norm.item() ** 2
                total_grad_norm = total_grad_norm ** 0.5
                
                if total_grad_norm > Config.grad_clip_norm * 10:
                    print(f"\nâš ï¸  æ¢¯åº¦å¼‚å¸¸å¤§è­¦å‘Š - Epoch {epoch+1}, Episode {episode_idx+1}:")
                    print(f"   æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.2f} (é˜ˆå€¼: {Config.grad_clip_norm})")
                    print(f"   å½“å‰å­¦ä¹ ç‡: {scheduler.get_lr():.6f}")
                
                optimizer.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            acc = compute_episode_accuracy(logits, query_labels)
            epoch_losses.append(total_loss.item())
            epoch_accuracies.append(acc)
            epoch_domain_losses.append(domain_loss.item())
            epoch_domain_accuracies.append(domain_acc)
            epoch_intrinsic_supcon_losses.append(intrinsic_supcon_loss.item())
            epoch_domain_supcon_losses.append(domain_supcon_loss.item())
            
            # ===== è®°å½•æœ¬å¾æ”¯ç»Ÿè®¡æŒ‡æ ‡ =====
            if intrinsic_supcon_stats:
                # è·¨åŸŸåŒç±»ç›¸ä¼¼åº¦ (æ ¸å¿ƒæŒ‡æ ‡ï¼Œå¯¹åº” cross_domain_weight=1.5 çš„æ­£æ ·æœ¬)
                if 'avg_cross_domain_sim' in intrinsic_supcon_stats:
                    epoch_cross_domain_sim.append(intrinsic_supcon_stats['avg_cross_domain_sim'])
                # åŒåŸŸåŒç±»ç›¸ä¼¼åº¦ (å¯¹åº” same_domain_weight=0.8 çš„æ­£æ ·æœ¬)
                if 'avg_same_domain_sim' in intrinsic_supcon_stats:
                    epoch_intrinsic_pos_sim.append(intrinsic_supcon_stats['avg_same_domain_sim'])
                # è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
                if 'avg_neg_sim' in intrinsic_supcon_stats:
                    epoch_intrinsic_neg_sim.append(intrinsic_supcon_stats['avg_neg_sim'])
            
            # ===== è®°å½•åŸŸæ”¯ç»Ÿè®¡æŒ‡æ ‡ =====
            if domain_supcon_stats:
                if 'avg_cross_class_sim' in domain_supcon_stats:
                    epoch_domain_cross_class_sim.append(domain_supcon_stats['avg_cross_class_sim'])
                if 'avg_same_class_sim' in domain_supcon_stats:
                    epoch_domain_same_class_sim.append(domain_supcon_stats['avg_same_class_sim'])
                if 'avg_cdsc_sim' in domain_supcon_stats:
                    epoch_domain_cdsc_sim.append(domain_supcon_stats['avg_cdsc_sim'])
                if 'avg_cddc_sim' in domain_supcon_stats:
                    epoch_domain_cddc_sim.append(domain_supcon_stats['avg_cddc_sim'])
                
                # ===== è®¡ç®—å¹¶è®°å½• LeakIndex (æ–°å¢) =====
                # åªæœ‰åŒæ—¶å­˜åœ¨ CDSC å’Œ CDDC æ—¶æ‰è®¡ç®—
                if 'avg_cdsc_sim' in domain_supcon_stats and 'avg_cddc_sim' in domain_supcon_stats:
                    leak_result = compute_leak_index(
                        cdsc=domain_supcon_stats['avg_cdsc_sim'],
                        cddc=domain_supcon_stats['avg_cddc_sim'],
                        mode='raw_diff'
                    )
                    epoch_leak_list.append(leak_result['leak_index'])
                else:
                    # ç¼ºå°‘å¿…è¦æ•°æ®æ—¶è®°å½• NaN
                    epoch_leak_list.append(np.nan)
            
            # ===== è®°å½• SAP ç»Ÿè®¡æŒ‡æ ‡ (æ–°å¢) =====
            if sap_stats:
                if 'gate' in sap_stats:
                    epoch_sap_gate.append(sap_stats['gate'])
                if 'attn_max' in sap_stats:
                    epoch_sap_attn_max.append(sap_stats['attn_max'])
                if 'purification_ratio' in sap_stats:
                    epoch_sap_purif_ratio.append(sap_stats['purification_ratio'])
                if 'w_orth_norm' in sap_stats:
                    epoch_sap_w_orth_norm.append(sap_stats['w_orth_norm'])
            
            # è®°å½•episodeæ—¶é—´
            episode_time = time.time() - episode_start_time
            episode_times.append(episode_time)

            if epoch_sep_ratios is not None:
                support_features, _ = model.extract_features(support_images)
                # support_features is already [B, 640] flattened vector from DualStreamResNet12
                sep_metrics = compute_prototype_separation_ratio(support_features, support_labels, prototypes)
                if 'separation_ratio' in sep_metrics:
                    r = sep_metrics['separation_ratio']
                    epoch_sep_ratios.append(float(r))
            

            
            # æ‰“å°è¿›åº¦ - åŒ…å«å¤šæŸå¤±ä¿¡æ¯ (SupCon + SAP)
            if (episode_idx + 1) % Config.log_interval == 0:
                avg_loss = np.mean(epoch_losses[-Config.log_interval:])
                avg_acc = np.mean(epoch_accuracies[-Config.log_interval:])
                avg_domain = np.mean(epoch_domain_losses[-Config.log_interval:])
                avg_domain_acc = np.mean(epoch_domain_accuracies[-Config.log_interval:])
                avg_intrinsic_supcon = np.mean(epoch_intrinsic_supcon_losses[-Config.log_interval:])
                avg_domain_supcon = np.mean(epoch_domain_supcon_losses[-Config.log_interval:])
                avg_time = np.mean(episode_times[-Config.log_interval:])
                
                # æœ¬å¾æ”¯ç›¸ä¼¼åº¦ç»Ÿè®¡
                avg_int_cross_sim = np.mean(epoch_cross_domain_sim[-Config.log_interval:]) if epoch_cross_domain_sim else 0.0
                avg_int_pos_sim = np.mean(epoch_intrinsic_pos_sim[-Config.log_interval:]) if epoch_intrinsic_pos_sim else 0.0
                avg_int_neg_sim = np.mean(epoch_intrinsic_neg_sim[-Config.log_interval:]) if epoch_intrinsic_neg_sim else 0.0
                
                # åŸŸæ”¯ç›¸ä¼¼åº¦ç»Ÿè®¡
                avg_dom_cdsc = np.mean(epoch_domain_cdsc_sim[-Config.log_interval:]) if epoch_domain_cdsc_sim else 0.0
                avg_dom_sddc = np.mean(epoch_domain_cross_class_sim[-Config.log_interval:]) if epoch_domain_cross_class_sim else 0.0
                avg_dom_sdsc = np.mean(epoch_domain_same_class_sim[-Config.log_interval:]) if epoch_domain_same_class_sim else 0.0
                
                # SAP ç»Ÿè®¡ä¿¡æ¯
                avg_sap_gate = np.mean(epoch_sap_gate[-Config.log_interval:]) if epoch_sap_gate else 0.0
                avg_sap_attn_max = np.mean(epoch_sap_attn_max[-Config.log_interval:]) if epoch_sap_attn_max else 0.0
                avg_sap_purif_ratio = np.mean(epoch_sap_purif_ratio[-Config.log_interval:]) if epoch_sap_purif_ratio else 0.0
                avg_sap_w_orth_norm = np.mean(epoch_sap_w_orth_norm[-Config.log_interval:]) if epoch_sap_w_orth_norm else 0.0

                print(f"  Episode {episode_idx+1}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}")
                print(f"    æœ¬å¾æ”¯ - IntSupCon={avg_intrinsic_supcon:.4f}, è·¨åŸŸåŒç±»={avg_int_cross_sim:.3f}, åŒåŸŸåŒç±»={avg_int_pos_sim:.3f}, å¼‚ç±»={avg_int_neg_sim:.3f}")
                print(f"    åŸŸæ”¯   - DomSupCon={avg_domain_supcon:.4f}, CDSCè·¨åŸŸåŒç±»={avg_dom_cdsc:.3f}, SDDCåŒåŸŸå¼‚ç±»={avg_dom_sddc:.3f}, SDSCåŒåŸŸåŒç±»={avg_dom_sdsc:.3f}")
                print(f"    SAPå‡€åŒ– - Gate={avg_sap_gate:.3f}, AttnMax={avg_sap_attn_max:.3f}, PurifRatio={avg_sap_purif_ratio:.3f}, W_orth={avg_sap_w_orth_norm:.2f}")
                print(f"    Time={avg_time:.2f}s")
                
        
        # è®¡ç®—epochç»Ÿè®¡ä¿¡æ¯ - åŒ…å«å¤šæŸå¤±ç»„ä»¶ (åŒå‘å¯¹æ¯”å­¦ä¹ )
        avg_epoch_loss = np.mean(epoch_losses)
        avg_epoch_acc = np.mean(epoch_accuracies)
        avg_epoch_domain = np.mean(epoch_domain_losses)
        avg_epoch_domain_acc = np.mean(epoch_domain_accuracies)
        avg_epoch_intrinsic_supcon = np.mean(epoch_intrinsic_supcon_losses)
        avg_epoch_domain_supcon = np.mean(epoch_domain_supcon_losses)
        avg_epoch_cross_sim = np.mean(epoch_cross_domain_sim) if epoch_cross_domain_sim else 0.0
        train_losses.append(avg_epoch_loss)
        train_accuracies.append(avg_epoch_acc)
        
        # ===== è®¡ç®—å¯¹æ¯”å­¦ä¹ ç»Ÿè®¡æŒ‡æ ‡å‡å€¼ =====
        # æœ¬å¾æ”¯æŒ‡æ ‡
        avg_intrinsic_pos_sim = np.mean(epoch_intrinsic_pos_sim) if epoch_intrinsic_pos_sim else 0.0
        avg_intrinsic_neg_sim = np.mean(epoch_intrinsic_neg_sim) if epoch_intrinsic_neg_sim else 0.0
        # åŸŸæ”¯æŒ‡æ ‡
        avg_domain_cross_class_sim = np.mean(epoch_domain_cross_class_sim) if epoch_domain_cross_class_sim else 0.0
        avg_domain_same_class_sim = np.mean(epoch_domain_same_class_sim) if epoch_domain_same_class_sim else 0.0
        avg_domain_cdsc_sim = np.mean(epoch_domain_cdsc_sim) if epoch_domain_cdsc_sim else 0.0
        avg_domain_cddc_sim = np.mean(epoch_domain_cddc_sim) if epoch_domain_cddc_sim else 0.0
        
        # æ·»åŠ åˆ°å…¨å±€å†å²è®°å½•
        intrinsic_pos_sim_history.append(avg_intrinsic_pos_sim)
        intrinsic_neg_sim_history.append(avg_intrinsic_neg_sim)
        intrinsic_cross_domain_sim_history.append(avg_epoch_cross_sim)
        domain_cross_class_sim_history.append(avg_domain_cross_class_sim)
        domain_same_class_sim_history.append(avg_domain_same_class_sim)
        domain_cdsc_sim_history.append(avg_domain_cdsc_sim)
        domain_cddc_sim_history.append(avg_domain_cddc_sim)
        
        # ===== è®¡ç®—å¹¶è®°å½• LeakIndex (epoch çº§åˆ«) =====
        if len(epoch_leak_list) > 0:
            # ä½¿ç”¨ nanmean å¤„ç†å¯èƒ½çš„ NaN å€¼
            avg_leak = float(np.nanmean(epoch_leak_list))
            std_leak = float(np.nanstd(epoch_leak_list))
            # LeakIntensity = max(0, LeakIndex)
            avg_intensity = float(np.nanmean([max(0.0, x) for x in epoch_leak_list if not np.isnan(x)]))
        else:
            # æ•´ä¸ª epoch æ²¡æœ‰æœ‰æ•ˆçš„ LeakIndex æ•°æ®
            avg_leak = np.nan
            std_leak = np.nan
            avg_intensity = np.nan
        
        leakindex_history.append(avg_leak)
        leakintensity_history.append(avg_intensity)
        
        # è®¡ç®—å¹¶è®°å½•epochç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€æ ‡å‡†è¯¯å·®ï¼‰
        epoch_mean, epoch_std, epoch_se = compute_epoch_statistics(epoch_accuracies)
        epoch_stds.append(epoch_std)
        
        # è®°å½•epochæ—¶é—´ç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        epoch_times.append(epoch_time)
        avg_episode_time = np.mean(episode_times)
        total_episode_time = np.sum(episode_times)
        
        # è·å–å½“å‰æœ¬å¾æ”¯SupConæ¸©åº¦
        intrinsic_supcon_temp = model.get_intrinsic_supcon_stats().get('temperature', 0.0) if hasattr(model, 'get_intrinsic_supcon_stats') else 0.0
        
        print(f"  Epoch Summary: Loss={avg_epoch_loss:.4f}, Acc={avg_epoch_acc:.4f}, Std={epoch_std:.4f}")
        print(f"  SupCon: Intrinsic={avg_epoch_intrinsic_supcon:.4f}, Domain={avg_epoch_domain_supcon:.4f}, Ï„={intrinsic_supcon_temp:.3f}")
        print(f"  æœ¬å¾æ”¯ç›¸ä¼¼åº¦: Pos={avg_intrinsic_pos_sim:.3f}, Neg={avg_intrinsic_neg_sim:.3f}, CrossDom={avg_epoch_cross_sim:.3f}")
        print(f"  åŸŸæ”¯ç›¸ä¼¼åº¦: SDDC={avg_domain_cross_class_sim:.3f}, SDSC={avg_domain_same_class_sim:.3f}, CDSC={avg_domain_cdsc_sim:.3f}, CDDC={avg_domain_cddc_sim:.3f}")
        # ===== LeakIndex æ—¥å¿—è¾“å‡º (æ–°å¢) =====
        if not np.isnan(avg_leak):
            print(f"  LeakIndex: {avg_leak:.4f}Â±{std_leak:.4f}, Intensity={avg_intensity:.4f}")
        else:
            print(f"  LeakIndex: N/A (æ— æœ‰æ•ˆæ•°æ®)")
        print(f"  Time Summary: Epoch={epoch_time:.2f}s, Avg Episode={avg_episode_time:.2f}s")
        
        # æ¯ 5 ä¸ª epoch è®°å½•ä¸€æ¬¡åˆ†ç¦»æ¯”å’ŒT-SNEå¯è§†åŒ–
        if (epoch + 1) % 5 == 0:
            # è‹¥æœ¬ epoch è®¡ç®—äº†åˆ†ç¦»æ¯”ï¼Œåˆ™ç´¯åŠ åˆ°æ›²çº¿æ•°æ®å¹¶æ‰“å°å‡å€¼
            if epoch_sep_ratios is not None and len(epoch_sep_ratios) > 0:
                avg_sep_ratio = float(np.mean(epoch_sep_ratios))
                sep_ratio_curve.append(avg_sep_ratio)
                sep_ratio_epochs.append(epoch + 1)
                print(f"  Separation Ratio (avg over epoch): {avg_sep_ratio:.4f}")
            
            # ===== T-SNE å¯è§†åŒ– (Refactored) =====
            tsne_metrics = tsne_visualizer.visualize(model, epoch + 1, tsne_save_dir)
            
            # è®°å½•T-SNEæŒ‡æ ‡
            tsne_metrics_history.append(tsne_metrics)
            tsne_epochs.append(epoch + 1)
            
            
        # æ ¹æ®é…ç½®å‚æ•°è¯„ä¼°éªŒè¯é›†æ€§èƒ½
        if (epoch + 1) % Config.eval_frequency == 0:
            print("  Evaluating on validation set...")
            val_acc, val_lower, val_upper = evaluate_model(
                model, val_dataset, Config, num_test_episodes=Config.val_episodes
            )
            val_accuracies.append(val_acc)
            print(f"  Validation Accuracy: {val_acc:.4f} ({val_lower:.4f} ~ {val_upper:.4f})")
            
            # æ”¶é›†çƒ­åŠ›å›¾æ•°æ®ï¼š[epoch, train_acc, val_acc, std]
            heatmap_data.append([epoch + 1, avg_epoch_acc, val_acc, epoch_std])
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best_model = False
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                is_best_model = True
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'model_name': model.__class__.__name__,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_val_acc': best_val_acc,
                }, 'best_model.pth')
                print(f"  Saved new best model with validation accuracy: {best_val_acc:.4f}")
            


            # æ¯ 10 ä¸ª epoch ç»˜åˆ¶ä¸€æ¬¡éªŒè¯å‡†ç¡®ç‡æ›²çº¿ï¼ˆè‹¥å·²æ”¶é›†åˆ°éªŒè¯ç»“æœï¼‰
            if (epoch + 1) % 10 == 0:
                # ç”Ÿæˆå·²è¯„ä¼°çš„ epoch åˆ—è¡¨
                current_val_epochs = [i * Config.eval_frequency for i in range(1, len(val_accuracies) + 1)]
                plot_val_accuracy_curve(
                    val_accuracies,
                    title=f"Validation Accuracy Curve (up to Epoch {epoch+1})",
                    val_epochs=current_val_epochs,
                    save_path=f"figures/val_accuracy_curve_epoch_{epoch+1}.png"
                )
                # è‹¥å·²è®¡ç®—åˆ†ç¦»æ¯”ï¼Œåˆ™åŒæ­¥ç»˜åˆ¶åˆ†ç¦»æ¯”å˜åŒ–æ›²çº¿
                if len(sep_ratio_curve) > 0:
                    plot_separation_ratio_curve(
                        sep_ratios=sep_ratio_curve,
                        sep_epochs=sep_ratio_epochs,
                        title=f"Separation Ratio Curve (up to Epoch {epoch+1})",
                        save_path=f"figures/separation_ratio_curve_epoch_{epoch+1}.png"
                    )
    

    
    # ç»˜åˆ¶æ”¹è¿›çš„è®­ç»ƒæ›²çº¿ - ä½¿ç”¨ä¸“ä¸šçš„å‡†ç¡®ç‡æ¯”è¾ƒå›¾
    # æ ¹æ®å®é™…éªŒè¯æ¬¡æ•°è®¡ç®—val_epochs
    val_epochs = list(range(Config.eval_frequency, len(val_accuracies) * Config.eval_frequency + 1, Config.eval_frequency))
    
    # ç»˜åˆ¶ä¼ ç»Ÿçš„è®­ç»ƒæ›²çº¿ï¼ˆåŒ…å«æŸå¤±ï¼‰
    plot_training_curve(train_losses, train_accuracies, val_accuracies, 
                       title="Complete Training Curve", val_epochs=val_epochs,
                       save_path="figures/complete_training_curve.png")
    
    # ç»˜åˆ¶è®­ç»ƒä¸éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”å›¾ï¼Œç›´è§‚å±•ç¤ºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸éªŒè¯é›†ä¸Šçš„æ€§èƒ½å·®å¼‚
    plot_accuracy_comparison(train_accuracies, val_accuracies, 
                            title="Training vs Validation Accuracy Comparison",
                            val_epochs=val_epochs,
                            save_path="figures/accuracy_comparison.png")
    # ç»˜åˆ¶è®­ç»ƒé˜¶æ®µæœ€ç»ˆç»Ÿè®¡å›¾ï¼Œå±•ç¤ºå„ epoch çš„å¹³å‡å‡†ç¡®ç‡ä¸æ ‡å‡†å·®
    plot_epoch_statistics(train_accuracies, epoch_stds,
                          title="Training Statistics (Final)",
                          save_path="figures/epoch_stats_final.png")
    # å•ç‹¬ç»˜åˆ¶è®­ç»ƒé˜¶æ®µå„ epoch å¹³å‡å‡†ç¡®ç‡æ›²çº¿ï¼Œä¾¿äºè§‚å¯Ÿæ•´ä½“è¶‹åŠ¿
    plot_epoch_accuracy(train_accuracies,
                        title="Epoch Average Accuracy (Final)",
                        save_path="figures/epoch_accuracy_final.png")
    
    # ç»˜åˆ¶å‡†ç¡®ç‡çƒ­åŠ›å›¾ - è¡Œä¸ºä¸åŒepochï¼Œåˆ—ä¸ºæŒ‡æ ‡
    if len(heatmap_data) > 0:
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼šå½¢çŠ¶ [n_epochs, 3] -> (train_acc, val_acc, std)
        heatmap_matrix = np.array(heatmap_data)[:, 1:]  # ä¸è½¬ç½®ï¼ŒæŒ‰ epoch ä¸ºè¡Œ
        
        # åˆ›å»ºæ ‡ç­¾
        epoch_labels = [f"Epoch {int(data[0])}" for data in heatmap_data]
        metric_labels = ["è®­ç»ƒå‡†ç¡®ç‡", "éªŒè¯å‡†ç¡®ç‡", "æ ‡å‡†å·®"]
        
        plot_accuracy_heatmap(
            heatmap_matrix,
            class_names=epoch_labels,
            metric_names=metric_labels,
            title="Training Progress Heatmap - Accuracy & Statistics",
            save_path="figures/training_progress_heatmap.png"
        )
        
        print(f"  Generated training progress heatmap with {len(heatmap_data)} validation points")
    
    # ===== T-SNE èšç±»æŒ‡æ ‡æ¼”åŒ–å›¾ =====
    if len(tsne_metrics_history) > 0:
        plot_tsne_evolution(
            metrics_history=tsne_metrics_history,
            epochs=tsne_epochs,
            save_dir=tsne_save_dir,
            title="T-SNE èšç±»æŒ‡æ ‡æ¼”åŒ– (ExpB1Model)"
        )
        print(f"  ğŸ“ˆ T-SNEèšç±»æŒ‡æ ‡æ¼”åŒ–å›¾å·²ç”Ÿæˆï¼Œå…± {len(tsne_epochs)} ä¸ªé‡‡æ ·ç‚¹")
    
    # ===== å¯¹æ¯”å­¦ä¹ ç›¸ä¼¼åº¦æ›²çº¿å¯è§†åŒ– =====
    import matplotlib.pyplot as plt
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    epochs_range = range(1, len(intrinsic_pos_sim_history) + 1)
    
    # æœ¬å¾æ”¯ç›¸ä¼¼åº¦æ›²çº¿
    ax1 = axes[0]
    ax1.plot(epochs_range, intrinsic_pos_sim_history, 'g-', label='åŒç±»æ­£æ ·æœ¬ (Pos)', linewidth=2)
    ax1.plot(epochs_range, intrinsic_neg_sim_history, 'r-', label='å¼‚ç±»è´Ÿæ ·æœ¬ (Neg)', linewidth=2)
    ax1.plot(epochs_range, intrinsic_cross_domain_sim_history, 'b--', label='è·¨åŸŸåŒç±» (CrossDom)', linewidth=2)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('ä½™å¼¦ç›¸ä¼¼åº¦')
    ax1.set_title('æœ¬å¾æ”¯ (Intrinsic) ç›¸ä¼¼åº¦æ¼”åŒ–')
    ax1.legend(loc='best')
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(-0.5, 1.0)
    
    # åŸŸæ”¯ç›¸ä¼¼åº¦æ›²çº¿
    ax2 = axes[1]
    ax2.plot(epochs_range, domain_cross_class_sim_history, 'g-', label='åŒåŸŸå¼‚ç±» (SDDC)', linewidth=2)
    ax2.plot(epochs_range, domain_same_class_sim_history, 'b-', label='åŒåŸŸåŒç±» (SDSC)', linewidth=2)
    ax2.plot(epochs_range, domain_cdsc_sim_history, 'm--', label='è·¨åŸŸåŒç±» (CDSC-Leak)', linewidth=2)
    ax2.plot(epochs_range, domain_cddc_sim_history, 'r:', label='è·¨åŸŸå¼‚ç±» (CDDC-Neg)', linewidth=2)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('ä½™å¼¦ç›¸ä¼¼åº¦')
    ax2.set_title('åŸŸæ”¯ (Domain) ç›¸ä¼¼åº¦è¡Œä¸ºåˆ†æ')
    ax2.legend(loc='best')
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(-0.5, 1.0)
    
    plt.tight_layout()
    plt.savefig('figures/supcon_similarity_evolution.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  ğŸ“ˆ å¯¹æ¯”å­¦ä¹ ç›¸ä¼¼åº¦æ›²çº¿å·²ä¿å­˜åˆ°: figures/supcon_similarity_evolution.png")
    
    # ===== LeakIndex å¯è§†åŒ– (æ–°å¢) =====
    if len(leakindex_history) > 0:
        # è¿‡æ»¤æ‰ NaN å€¼ä»¥ä¾¿ç»˜å›¾
        valid_indices = [i for i, x in enumerate(leakindex_history) if not np.isnan(x)]
        if len(valid_indices) > 0:
            valid_epochs = [i + 1 for i in valid_indices]
            valid_leakindex = [leakindex_history[i] for i in valid_indices]
            valid_leakintensity = [leakintensity_history[i] for i in valid_indices]
            
            plot_leakindex_curve(
                leakindex_history=valid_leakindex,
                leakintensity_history=valid_leakintensity,
                epochs=valid_epochs,
                title="LeakIndex æ¼”åŒ–æ›²çº¿ (åŸŸæ”¯è¯­ä¹‰æ³„éœ²ç›‘æ§)",
                save_path="figures/leakindex_evolution.png"
            )
            print(f"  ğŸ“Š LeakIndex æ›²çº¿å·²ä¿å­˜åˆ°: figures/leakindex_evolution.png")
            print(f"     æœ‰æ•ˆæ•°æ®ç‚¹: {len(valid_indices)}/{len(leakindex_history)} epochs")
        else:
            print(f"  âš ï¸  LeakIndex æ•°æ®å…¨éƒ¨ä¸º NaNï¼Œè·³è¿‡å¯è§†åŒ–")
    else:
        print(f"  âš ï¸  LeakIndex å†å²è®°å½•ä¸ºç©ºï¼Œè·³è¿‡å¯è§†åŒ–")
    
    # è®¡ç®—å¹¶è¾“å‡ºæ€»è®­ç»ƒæ—¶é—´ç»Ÿè®¡
    total_training_time = time.time() - total_start_time
    avg_epoch_time = np.mean(epoch_times)
    total_episodes = Config.num_epochs * Config.episodes_per_epoch
    avg_episode_time_overall = total_training_time / total_episodes
    
    print("\n" + "="*60)
    print("TRAINING COMPLETED - TIME STATISTICS")
    print("="*60)
    print(f"Total Training Time: {total_training_time:.2f}s ({total_training_time/3600:.2f}h)")
    print(f"Number of Epochs: {Config.num_epochs}")
    print(f"Average Time per Epoch: {avg_epoch_time:.2f}s")
    print(f"Total Episodes: {total_episodes}")
    print(f"Average Time per Episode: {avg_episode_time_overall:.2f}s")
    print(f"Episodes per Epoch: {Config.episodes_per_epoch}")
    print("="*60)
    print("All done!")


if __name__ == "__main__":
    main()
